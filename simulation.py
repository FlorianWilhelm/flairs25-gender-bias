from enum import Enum, auto
from dataclasses import dataclass
import hashlib

import numpy as np
from numpy.random import Generator
import pandas as pd
from tqdm.auto import tqdm
from scipy.optimize import linprog, OptimizeResult
from scipy.stats import dirichlet, expon, beta, gamma
from scipy.stats._multivariate import dirichlet_frozen


class QuotaType(Enum):
    "Enumeration for the different types of quotas"
    NONE = auto()
    GTE20 = 0.2
    GTE30 = 0.3
    GTE40 = 0.4
    EQU50 = auto()


@dataclass
class Params:
    alpha: float
    n_roles: int
    n_persons: int
    rng: Generator


@dataclass
class Data:
    params: Params
    prefs: np.ndarray
    genders: np.ndarray
    caps: np.ndarray
    tvd: float

    def __hash__(self):
        prefs_hash = hashlib.sha256(self.prefs.tobytes()).hexdigest()
        genders_hash = hashlib.sha256(self.genders.tobytes()).hexdigest()
        caps_hash = hashlib.sha256(self.caps.tobytes()).hexdigest()
        return hash((prefs_hash, genders_hash, caps_hash, self.tvd))

@dataclass
class Experiment:
    data: Data
    quota: QuotaType
    res: OptimizeResult

    @property
    def is_good(self) -> bool:
        return self.res.success
    
    @property
    def alloc(self) -> np.ndarray:
        return self.res.x.reshape(self.data.params.n_persons, self.data.params.n_roles)
    
    @property
    def person_util(self) -> np.ndarray:
        return (self.data.prefs * self.alloc).sum(axis=1)

    @property
    def total_util(self) -> float:
        return -self.res.fun
    
    @property
    def g0_util(self) -> float:
        return (self.person_util * (1 - self.data.genders)).sum()

    @property
    def g1_util(self) -> float:
        return (self.person_util * self.data.genders).sum()
    

@dataclass
class Simulation:
    data: Data
    experiments: list[Experiment]
    
    @property
    def id(self) -> str:
        return str(abs(hash(self.data)))[:8]


def log_safe(x: np.ndarray, epsilon:float =1e-12) -> np.ndarray:
    return np.log(np.maximum(x, epsilon))


def calc_tvd(p: np.ndarray, q: np.ndarray) -> float: 
    "Total variation distance between two probability distributions"
    return 0.5 * np.sum(np.abs(p - q))


def sample_exp_dist(size: int, lambda_: float, rng: Generator | int | None=None) -> np.ndarray:
    return expon.rvs(scale=1/lambda_, size=size, random_state=rng)


def sample_gamma_dist(size: int, alpha: float, theta: float = 1., rng: Generator | int | None=None) -> np.ndarray:
    return gamma.rvs(alpha, scale=theta, size=size, random_state=rng)


def gen_alphas(dim: int, alpha: float, tvd: float, n_candidates:int=10_000, rng: Generator | int | None=None) -> tuple[np.ndarray, np.ndarray]:
    """Alpha is the expectation value of the distribution as theta = 1"""
    assert 0 <= tvd < 1, "TVD target must be in the range [0, 1)."
    rng = np.random.default_rng(seed=rng)

    if tvd == 0:
        alpha = sample_gamma_dist(size=dim, alpha=alpha, rng=rng)
        return alpha, alpha

    def sample_alphas() -> np.ndarray:
        alpha_g1 = sample_gamma_dist(size=dim, alpha=alpha, rng=rng)
        alpha_g0 = sample_gamma_dist(size=dim, alpha=alpha, rng=rng)
        mean_g0 = alpha_g0 / np.sum(alpha_g0)
        mean_g1 = alpha_g1 / np.sum(alpha_g1)
        return calc_tvd(mean_g0, mean_g1), (alpha_g0, alpha_g1)

    candidates = [sample_alphas() for _ in range(n_candidates)]
    cand_tvds = np.array([cand[0] for cand in candidates])
    index = np.argmin(np.abs(cand_tvds - tvd))
    return candidates[index][1]


def gen_gender_prefs(num: int, dist_g0: dirichlet_frozen, dist_g1: dirichlet_frozen, rng: Generator | int | None=None) -> tuple[np.ndarray, np.ndarray]:
    half = num // 2
    rest = num - half
    preferences = np.vstack((
        dist_g0.rvs(size=half, random_state=rng),
        dist_g1.rvs(size=rest, random_state=rng)
    ))
    genders = np.array([0] * half + [1] * rest)
    return preferences, genders


def stick_breaking(alpha, dim, rng: Generator | int | None=None) -> np.ndarray:
    """
    Perform the stick-breaking process for a Dirichlet Process.
    
    Parameters:
        alpha (float): Concentration parameter of the Dirichlet Process.
        num_components (int): Number of components (or sticks) to generate.
        random_state (int, np.random.Generator, optional): Random state for reproducibility.

    Returns:
        np.ndarray: Weights generated by the stick-breaking process.
    """
    beta_samples = beta.rvs(1, alpha, size=dim, random_state=rng)
    remaining_stick = 1.0
    weights = []

    for sample in beta_samples:
        weight = remaining_stick * sample
        weights.append(weight)
        remaining_stick *= (1 - sample)

    return np.array(weights)


def break_by_weights(total: int, weights: np.ndarray) -> np.ndarray:
    """
    Break a total number of items into components according to the given weights.
    
    Parameters:
        total (int): Total number of items to break.
        weights (np.ndarray): Weights for each component.
    
    Returns:
        np.ndarray: Number of items in each component.
    """
    weights = weights / np.sum(weights)
    float_parts = weights * total
    int_parts = np.floor(float_parts).astype(int)
    remainder = total - int_parts.sum()
    residuals = float_parts - int_parts
    indices = np.argsort(-residuals)[:remainder]
    int_parts[indices] += 1
    assert int_parts.sum() == total
    return int_parts


def gen_capacities(n_cap: int, total_cap: int, alpha: float, rng: Generator | int | None=None) -> np.ndarray:
    rng = np.random.default_rng(seed=rng)
    while True:
        weights = rng.permutation(stick_breaking(alpha, n_cap, rng))
        caps = break_by_weights(total_cap, weights)
        if np.all(caps > 0):
            break
    return caps


def allocate_roles(prefs: np.ndarray, genders: np.ndarray, caps: np.ndarray, quota: QuotaType) -> OptimizeResult:
    n_persons, n_roles = prefs.shape
    assert n_roles == caps.size
    assert n_persons == genders.size

    # Define linear program
    ## objective function: minimize -preferences
    c = -prefs.flatten()
    bounds = (0, 1)
    ## inequality constraints: sum of persons in roles <= capacities of roles
    A_ub = np.zeros((n_roles, n_roles * n_persons))
    for i in range(n_roles):
        A_ub[i, i::n_roles] = 1
    b_ub = caps
    ## equality constraints: each person gets exactly one role
    # ToDo: Check if we shoud rather make this a uber bound as people could also have no role.
    A_eq = np.zeros((n_persons, n_roles * n_persons))
    for i in range(n_persons):
        A_eq[i, i * n_roles:(i+1) * n_roles] = 1
    b_eq = np.ones(n_persons)

    match quota:
        case QuotaType.NONE:
            pass
        case QuotaType.EQU50:
            # equality constraints: exactly 50% for each gender in each role 
            A_eq_50 = np.zeros((n_roles, n_roles * n_persons))
            for i in range(n_roles):
                A_eq_50[i, i::n_roles] = genders
            A_eq = np.vstack((A_eq, A_eq_50))
            b_eq_50 = break_by_weights(genders.sum(), caps/caps.sum())
            b_eq = np.hstack((b_eq, b_eq_50))
        case QuotaType() as quota:
            # inequality constraints: at least x0% of gender 1 in each role
            A_ub_20 = np.zeros((n_roles, n_roles * n_persons))
            for i in range(n_roles):
                A_ub_20[i, i::n_roles] = quota.value - genders
            A_ub = np.vstack((A_ub, A_ub_20))
            b_ub_20 = np.zeros(n_roles)
            b_ub = np.hstack((b_ub, b_ub_20))
        
    res = linprog(c, A_ub=A_ub, b_ub=b_ub, A_eq=A_eq, b_eq=b_eq, bounds=bounds, integrality=1)

    return res

def generate_data(alpha: float, n_roles: int, n_persons: int, tvd: float | None = None, rng: Generator | int | None = None) -> Data:
    rng = np.random.default_rng(seed=rng)
    tvd = tvd or rng.uniform(0, 1)
    assert 0 <= tvd <= 1

    alpha_g0, alpha_g1 = gen_alphas(dim=n_roles, alpha=alpha, tvd=tvd, rng=rng)
    prior_g0 = dirichlet(alpha_g0)
    prior_g1 = dirichlet(alpha_g1)
    prefs, genders = gen_gender_prefs(num=n_persons, dist_g0=prior_g0, dist_g1=prior_g1)
    tvd = calc_tvd(prior_g0.mean(), prior_g1.mean())
    caps = gen_capacities(n_cap=n_roles, total_cap=n_persons, alpha=1, rng=rng)
    params = Params(alpha=alpha, n_roles=n_roles, n_persons=n_persons, rng=rng)
    return Data(params=params, prefs=prefs, genders=genders, caps=caps, tvd=tvd)


def run_experiment(data: Data, quota: QuotaType) -> Experiment:
    res = allocate_roles(prefs=data.prefs, genders=data.genders, caps=data.caps, quota=quota)
    return Experiment(data=data, quota=quota, res=res)


def run_simulation(alpha: float, n_roles: int, n_persons: int, n_sims: int =1, rng: Generator | None = None) -> list[Simulation]:
    simulations = []
    with tqdm(total=n_sims*len(QuotaType), desc="Simulations") as progress:
        for _ in range(n_sims):
            data = generate_data(alpha=alpha, n_roles=n_roles, n_persons=n_persons, rng=rng)
            experiments = []
            for quota in QuotaType:
                experiments.append(run_experiment(data, quota))
                progress.update(1)
            simulations.append(Simulation(data=data, experiments=experiments))
    return simulations

def make_df(simulations: list[Simulation]) -> pd.DataFrame:
    rows = []
    for sim in simulations:
        for exp in sim.experiments:
            row = {
                "id": sim.id,
                "lambda": sim.data.params.alpha,
                "n_roles": sim.data.params.n_roles,
                "n_persons": sim.data.params.n_persons,
                "tvd": sim.data.tvd,
                "quota": exp.quota,
                "success": exp.is_good,
                "total_util": exp.total_util,
                "g0_util": exp.g0_util,
                "g1_util": exp.g1_util
            }
            rows.append(row)
    df = pd.DataFrame(rows)

    def add_util_perc(df: pd.DataFrame, col:str ) -> pd.DataFrame:
        none_util = df[df['quota'].apply(lambda x: x.name) == QuotaType.NONE.name].set_index('id')[col]
        return df[col] / df['id'].map(none_util)

    df['total_util_perc'] = add_util_perc(df, 'total_util')
    df['g0_util_perc'] = add_util_perc(df, 'g0_util')
    df['g1_util_perc'] = add_util_perc(df, 'g1_util')
    return df
